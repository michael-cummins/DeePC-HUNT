{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.parameter import Parameter\n",
    "import torch.optim as optim\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from mpc import mpc\n",
    "from mpc.mpc import QuadCost\n",
    "\n",
    "from IPython.core import ultratb\n",
    "\n",
    "from mpc.dynamics import AffineDynamics\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter Initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_batch, n_state, n_ctrl, T = 24, 3, 3, 10\n",
    "n_sc = n_state + n_ctrl\n",
    "device = 'cpu'\n",
    "u_lower = torch.tensor([-0.5,-0.5,-0.5], dtype=torch.float32)\n",
    "u_lower = u_lower.repeat(T, n_batch, 1)\n",
    "u_upper = torch.tensor([0.5,0.5,0.5], dtype=torch.float32)\n",
    "u_upper = u_upper.repeat(T, n_batch, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([10.0000, 10.0000, 10.0000,  0.1000,  0.1000,  0.1000])\n"
     ]
    }
   ],
   "source": [
    "goal_state = torch.Tensor([2,1,-1])\n",
    "goal_weights = torch.ones(n_state)*10\n",
    "px = -(goal_weights)*goal_state\n",
    "p = torch.cat((px, torch.zeros(n_ctrl)))\n",
    "p = p.unsqueeze(0).repeat(T, n_batch, 1)\n",
    "\n",
    "ctr_penalty = 0.1\n",
    "q = torch.cat([goal_weights, torch.ones(n_ctrl)*ctr_penalty]).to(device)\n",
    "print(q)\n",
    "Q = torch.diag(q).unsqueeze(0).unsqueeze(0).repeat(\n",
    "        T, n_batch, 1, 1\n",
    ").to(device)\n",
    "A = torch.tensor([[1.01, 0.01, 0],[0.01, 1.01, 0.01],[0, 0.01, 1.01]]).to(device)\n",
    "B = torch.eye(3).to(device)\n",
    "\n",
    "# Initialise Parameters\n",
    "weight_est, ctrl_est = Parameter(torch.randn(size=(3,))*0.1+1), Parameter(torch.randn(size=(3,))*0.1+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8732266426086426\n",
      "Parameter containing:\n",
      "tensor([1.0178, 0.8498, 1.0057], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(weight_est.sum().item())\n",
    "print(weight_est)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([10.0000, 10.0000, 10.0000,  0.1000,  0.1000,  0.1000])\n",
      "tensor([1.0178, 0.8498, 1.0057, 0.9706, 0.9906, 0.9783],\n",
      "       grad_fn=<CatBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(q)\n",
    "print(torch.cat([weight_est, ctrl_est]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Function Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss(x_init : torch.Tensor, q_est : torch.Tensor, r_est : torch.Tensor) -> torch.Tensor:\n",
    "\n",
    "        # Expert \n",
    "        x_true, u_true, objs_true = mpc.MPC(\n",
    "            n_state, n_ctrl, T,\n",
    "            u_lower=u_lower, u_upper=u_upper, \n",
    "            lqr_iter=100,\n",
    "            verbose=-1,\n",
    "            exit_unconverged=False,\n",
    "            detach_unconverged=False,\n",
    "            backprop=False,\n",
    "            n_batch=n_batch,\n",
    "        )(x_init, QuadCost(Q, p), AffineDynamics(A=A, B=B))\n",
    "\n",
    "        # Learner\n",
    "\n",
    "        # Construct cost matrices from ctrl and state penalty\n",
    "        # Weights and penalties are identical for each state so \n",
    "        # We only need to optimize over two scalar variables \"weight_est\" and \"ctrl_est\"\n",
    "\n",
    "        q = torch.cat([q_est, r_est])\n",
    "        Q_est = torch.diag(q).unsqueeze(0).unsqueeze(0).repeat(\n",
    "                T, n_batch, 1, 1\n",
    "        ).to(device)\n",
    "        px = -(q_est)*goal_state\n",
    "        p_est = torch.cat((px, torch.zeros(n_ctrl)))\n",
    "        p_est = p_est.unsqueeze(0).repeat(T, n_batch, 1)    \n",
    "\n",
    "        # Roll out MPC with estimated cost function\n",
    "        x_pred, u_pred, objs_pred = mpc.MPC(\n",
    "            n_state, n_ctrl, T,\n",
    "            u_lower=u_lower, u_upper=u_upper, \n",
    "            lqr_iter=100,\n",
    "            verbose=-1,\n",
    "            backprop=False,\n",
    "            exit_unconverged=False,\n",
    "            detach_unconverged=False,\n",
    "            n_batch=n_batch,\n",
    "        )(x_init, QuadCost(Q_est, p_est), AffineDynamics(A=A, B=B))\n",
    "\n",
    "        # Get MSE of trajectory\n",
    "        criterion = torch.nn.MSELoss()\n",
    "        traj_loss = criterion(input=u_pred, target=u_true)\n",
    "        # traj_loss = torch.mean((u_true - u_pred)**2) #+ torch.mean((x_true - x_pred)**2)\n",
    "        return traj_loss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                            | 0/50 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "LQRStep.<locals>.LQRStepFn.backward() takes from 3 to 5 positional arguments but 7 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[95], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m loss \u001b[39m=\u001b[39m get_loss(x_init, weight_est, ctrl_est)\n\u001b[1;32m      8\u001b[0m opt\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m----> 9\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     10\u001b[0m opt\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     12\u001b[0m \u001b[39m# Used to checj the difference in ratio of ctrl cost and state cost\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages/torch/_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    388\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    389\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    390\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    394\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[1;32m    395\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[0;32m--> 396\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    175\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages/torch/autograd/function.py:253\u001b[0m, in \u001b[0;36mBackwardCFunction.apply\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mImplementing both \u001b[39m\u001b[39m'\u001b[39m\u001b[39mbackward\u001b[39m\u001b[39m'\u001b[39m\u001b[39m and \u001b[39m\u001b[39m'\u001b[39m\u001b[39mvjp\u001b[39m\u001b[39m'\u001b[39m\u001b[39m for a custom \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    250\u001b[0m                        \u001b[39m\"\u001b[39m\u001b[39mFunction is not allowed. You should only implement one \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    251\u001b[0m                        \u001b[39m\"\u001b[39m\u001b[39mof them.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    252\u001b[0m user_fn \u001b[39m=\u001b[39m vjp_fn \u001b[39mif\u001b[39;00m vjp_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m Function\u001b[39m.\u001b[39mvjp \u001b[39melse\u001b[39;00m backward_fn\n\u001b[0;32m--> 253\u001b[0m \u001b[39mreturn\u001b[39;00m user_fn(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs)\n",
      "\u001b[0;31mTypeError\u001b[0m: LQRStep.<locals>.LQRStepFn.backward() takes from 3 to 5 positional arguments but 7 were given"
     ]
    }
   ],
   "source": [
    "opt = optim.RMSprop((weight_est, ctrl_est), lr=1e-2)\n",
    "pbar = tqdm(range(50), ncols=120)\n",
    "\n",
    "for i in pbar:\n",
    "    x_init = torch.randn(n_batch,n_state)\n",
    "\n",
    "    loss = get_loss(x_init, weight_est, ctrl_est)\n",
    "    opt.zero_grad()\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "\n",
    "    # Used to checj the difference in ratio of ctrl cost and state cost\n",
    "    model_loss = np.abs(100 - weight_est.sum().item() / ctrl_est.sum().item())\n",
    "\n",
    "    pbar.set_description(f'Loss = {loss.item():.10f}, Model Loss = {model_loss:.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2609.1292, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([0.0149, 0.0153, 0.0151], requires_grad=True)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ctrl_est"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([1.5018, 1.4442, 1.5167], requires_grad=True)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight_est"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

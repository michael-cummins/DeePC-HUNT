{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from numpy import genfromtxt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from controllers import DDeePC\n",
    "plt.rcParams[\"figure.figsize\"] = (15,10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ud = genfromtxt('recht_ud.csv', delimiter=',')\n",
    "yd = genfromtxt('recht_yd.csv', delimiter=',')\n",
    "\n",
    "n = 3 # n = number of states\n",
    "m = 3 # m = number of inputs\n",
    "p = 3 # p = number of output\n",
    "q = m+p # q = number of i/o variables\n",
    "\n",
    "Tini = 4                                   # Past time horizon                                           \n",
    "Tf = 10                                     # Future time horizon         \n",
    "T = (m+1)*(Tini + Tf + n) - 1    \n",
    "noise_std = 0.1              \n",
    "\n",
    "A = torch.Tensor([[1.01, 0.01, 0.00], # A - State-space matrix\n",
    "                  [0.01, 1.01, 0.01], \n",
    "                  [0.00, 0.01, 1.01]])\n",
    "\n",
    "yd += np.random.rand(T,p)*noise_std\n",
    "u_ini = ud[:Tini,:].reshape((Tini*m,))\n",
    "y_ini = yd[:Tini,:].reshape((Tini*p,))\n",
    "y_constraints = np.kron(np.ones(Tf), np.array([100,100,100]))\n",
    "u_constraints = np.kron(np.ones(Tf), np.array([100,100,100]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialising Controllers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([103.4688, 100.5294,  99.7877], device='mps:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([100.2726, 100.8733,  99.6218], device='mps:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.0101], device='mps:0', requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "q = torch.ones(3)*200\n",
    "r = torch.ones(3)*0.02\n",
    "lam_y = torch.Tensor([0.2])\n",
    "\n",
    "expert = DDeePC(\n",
    "    ud=ud, yd=yd, N=Tf, Tini=Tini, T=T, p=3, m=3, n_batch=1,\n",
    "    y_constraints=y_constraints, u_constraints=u_constraints,\n",
    "    stochastic=True, linear=True, q=q, r=r, lam_y=lam_y\n",
    ")\n",
    "\n",
    "learner = DDeePC(\n",
    "    ud=ud, yd=yd, N=Tf, Tini=Tini, T=T, p=3, m=3, n_batch=1,\n",
    "    y_constraints=y_constraints, u_constraints=u_constraints,\n",
    "    stochastic=True, linear=True\n",
    ")\n",
    "\n",
    "for param in learner.parameters():\n",
    "    print(param)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Two or more parameters are on different devices. Expected parameter 2 to be on device mps:0 but got device cpu.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 17\u001b[0m\n\u001b[1;32m     13\u001b[0m expert_u, expert_y \u001b[39m=\u001b[39m [\u001b[39m*\u001b[39mexpert_u_ini], [\u001b[39m*\u001b[39mexpert_y_ini]\n\u001b[1;32m     15\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(T):\n\u001b[1;32m     16\u001b[0m     \u001b[39m# Solve for optimal actions\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m     u_pred, y_pred , _ \u001b[39m=\u001b[39m learner(ref\u001b[39m=\u001b[39;49mtest_ref, u_ini\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mTensor(learner_u_ini), y_ini\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mTensor(learner_y_ini))\n\u001b[1;32m     18\u001b[0m     u, y, _ \u001b[39m=\u001b[39m expert(ref\u001b[39m=\u001b[39mtest_ref, u_ini\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mTensor(expert_u_ini), y_ini\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mTensor(expert_y_ini))\n\u001b[1;32m     20\u001b[0m     \u001b[39m# Roll sim forward\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/differentiable-deepc/src/controllers.py:325\u001b[0m, in \u001b[0;36mDDeePC.forward\u001b[0;34m(self, ref, u_ini, y_ini)\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstochastic:\n\u001b[1;32m    323\u001b[0m     params\u001b[39m.\u001b[39mappend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlam_y)\n\u001b[0;32m--> 325\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mQP_layer(\u001b[39m*\u001b[39;49mparams)\n\u001b[1;32m    326\u001b[0m \u001b[39minput\u001b[39m, output \u001b[39m=\u001b[39m out[\u001b[39m2\u001b[39m], out[\u001b[39m3\u001b[39m]\n\u001b[1;32m    328\u001b[0m traj_cost \u001b[39m=\u001b[39m \u001b[39minput\u001b[39m\u001b[39m.\u001b[39mT \u001b[39m@\u001b[39m R \u001b[39m@\u001b[39m \u001b[39minput\u001b[39m \u001b[39m+\u001b[39m (output \u001b[39m-\u001b[39m ref)\u001b[39m.\u001b[39mT \u001b[39m@\u001b[39m Q \u001b[39m@\u001b[39m (output \u001b[39m-\u001b[39m ref)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages/cvxpylayers-0.1.5-py3.10.egg/cvxpylayers/torch/cvxpylayer.py:153\u001b[0m, in \u001b[0;36mCvxpyLayer.forward\u001b[0;34m(self, solver_args, *params)\u001b[0m\n\u001b[1;32m    140\u001b[0m info \u001b[39m=\u001b[39m {}\n\u001b[1;32m    141\u001b[0m f \u001b[39m=\u001b[39m _CvxpyLayerFn(\n\u001b[1;32m    142\u001b[0m     param_order\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparam_order,\n\u001b[1;32m    143\u001b[0m     param_ids\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparam_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    151\u001b[0m     info\u001b[39m=\u001b[39minfo,\n\u001b[1;32m    152\u001b[0m )\n\u001b[0;32m--> 153\u001b[0m sol \u001b[39m=\u001b[39m f(\u001b[39m*\u001b[39;49mparams)\n\u001b[1;32m    154\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minfo \u001b[39m=\u001b[39m info\n\u001b[1;32m    155\u001b[0m \u001b[39mreturn\u001b[39;00m sol\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages/torch/autograd/function.py:506\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    503\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m torch\u001b[39m.\u001b[39m_C\u001b[39m.\u001b[39m_are_functorch_transforms_active():\n\u001b[1;32m    504\u001b[0m     \u001b[39m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[1;32m    505\u001b[0m     args \u001b[39m=\u001b[39m _functorch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[0;32m--> 506\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mapply(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    508\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39msetup_context \u001b[39m==\u001b[39m _SingleLevelFunction\u001b[39m.\u001b[39msetup_context:\n\u001b[1;32m    509\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m    510\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    511\u001b[0m         \u001b[39m'\u001b[39m\u001b[39m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    512\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mstaticmethod. For more details, please see \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    513\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mhttps://pytorch.org/docs/master/notes/extending.func.html\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages/cvxpylayers-0.1.5-py3.10.egg/cvxpylayers/torch/cvxpylayer.py:197\u001b[0m, in \u001b[0;36m_CvxpyLayerFn.<locals>._CvxpyLayerFnFn.forward\u001b[0;34m(ctx, *params)\u001b[0m\n\u001b[1;32m    190\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    191\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mTwo or more parameters have different dtypes. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    192\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mExpected parameter \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m to have dtype \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m but \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    193\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mgot dtype \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m\n\u001b[1;32m    194\u001b[0m         (i, \u001b[39mstr\u001b[39m(ctx\u001b[39m.\u001b[39mdtype), \u001b[39mstr\u001b[39m(p\u001b[39m.\u001b[39mdtype))\n\u001b[1;32m    195\u001b[0m     )\n\u001b[1;32m    196\u001b[0m \u001b[39mif\u001b[39;00m p\u001b[39m.\u001b[39mdevice \u001b[39m!=\u001b[39m ctx\u001b[39m.\u001b[39mdevice:\n\u001b[0;32m--> 197\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    198\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mTwo or more parameters are on different devices. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    199\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mExpected parameter \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m to be on device \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    200\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mbut got device \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m\n\u001b[1;32m    201\u001b[0m         (i, \u001b[39mstr\u001b[39m(ctx\u001b[39m.\u001b[39mdevice), \u001b[39mstr\u001b[39m(p\u001b[39m.\u001b[39mdevice))\n\u001b[1;32m    202\u001b[0m     )\n\u001b[1;32m    204\u001b[0m \u001b[39m# check and extract the batch size for the parameter\u001b[39;00m\n\u001b[1;32m    205\u001b[0m \u001b[39m# 0 means there is no batch dimension for this parameter\u001b[39;00m\n\u001b[1;32m    206\u001b[0m \u001b[39m# and we assume the batch dimension is non-zero\u001b[39;00m\n\u001b[1;32m    207\u001b[0m \u001b[39mif\u001b[39;00m p\u001b[39m.\u001b[39mndimension() \u001b[39m==\u001b[39m q\u001b[39m.\u001b[39mndim:\n",
      "\u001b[0;31mValueError\u001b[0m: Two or more parameters are on different devices. Expected parameter 2 to be on device mps:0 but got device cpu."
     ]
    }
   ],
   "source": [
    "# Random reference\n",
    "test_ref = torch.Tensor(np.random.uniform(size=(3,), low=-5.0, high=5.0))\n",
    "# test_ref = torch.Tensor([3,0,-3])\n",
    "test_ref = torch.kron(torch.ones(Tf), test_ref)\n",
    "\n",
    "# Random initial condition\n",
    "index = int(np.random.uniform(low=0, high=5))\n",
    "learner_u_ini, learner_y_ini = ud[index:Tini + index, :].reshape((Tini*m,)), yd[index:Tini + index].reshape((Tini*p,))\n",
    "expert_u_ini, expert_y_ini = learner_u_ini, learner_y_ini\n",
    "\n",
    "T = 20\n",
    "learner_u, learner_y = [*learner_u_ini], [*learner_y_ini]\n",
    "expert_u, expert_y = [*expert_u_ini], [*expert_y_ini]\n",
    "\n",
    "for i in range(T):\n",
    "    # Solve for optimal actions\n",
    "    u_pred, y_pred , _ = learner(ref=test_ref, u_ini=torch.Tensor(learner_u_ini), y_ini=torch.Tensor(learner_y_ini))\n",
    "    u, y, _ = expert(ref=test_ref, u_ini=torch.Tensor(expert_u_ini), y_ini=torch.Tensor(expert_y_ini))\n",
    "\n",
    "    # Roll sim forward\n",
    "    learner_action, expert_action = u_pred[:p], u[:p]\n",
    "    learner_obs = A@learner_y_ini[-p:] + learner_action #+ noise \n",
    "    expert_obs = A@expert_y_ini[-p:] + expert_action #+ noise\n",
    "\n",
    "    # Collect data\n",
    "    learner_u, learner_y = np.append(learner_u, learner_action.detach().numpy()), np.append(learner_y, learner_obs.detach().numpy())\n",
    "    expert_u, expert_y = np.append(expert_u, expert_action.detach().numpy()), np.append(expert_y, expert_obs.detach().numpy())\n",
    "    learner_u_ini, learner_y_ini = learner_u[-m*Tini:], learner_y[-p*Tini:]\n",
    "    expert_u_ini, expert_y_ini = expert_u[-m*Tini:], expert_y[-p*Tini:]\n",
    "    \n",
    "\n",
    "learner_y = learner_y[p*Tini:].reshape((T, 3))\n",
    "expert_y = expert_y[p*Tini:].reshape((T, 3))\n",
    "learner_u = learner_u[p*Tini:].reshape((T, 3))\n",
    "expert_u = expert_u[p*Tini:].reshape((T, 3))\n",
    "# test_ref = test_ref.detach().numpy().reshape((Tf, 3))\n",
    "test_ref = np.kron(np.ones(T), test_ref[:p].detach().numpy()).reshape((T,3))\n",
    "\n",
    "plt.subplot(2,2,1)\n",
    "plt.title(f'Expert Trajectory - lam_y = {expert.lam_y.item():.3f}, q = {expert.q}')\n",
    "plt.grid('on')\n",
    "plt.plot(range(T), expert_y)\n",
    "plt.plot(range(T), test_ref, 'k--')\n",
    "plt.subplot(2,2,2)\n",
    "plt.title(f'Expert action - r = {expert.r}')\n",
    "plt.grid('on')\n",
    "plt.plot(range(T), expert_u)\n",
    "plt.subplot(2,2,3)\n",
    "plt.title(f'Learner Trajectory - lam_y = {learner.lam_y.item():.3f}, q = {learner.q.data}')\n",
    "plt.grid('on')\n",
    "plt.plot(range(T), learner_y)\n",
    "plt.plot(range(T), test_ref, 'k--')\n",
    "plt.subplot(2,2,4)\n",
    "plt.title(f'Learner Action - r = {learner.r.data}')\n",
    "plt.grid('on')\n",
    "plt.plot(range(T), learner_u)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss = 0.0111, lam_y = 4.661, q = tensor([15.8412, 17.0443, 14.4470]), r = tensor([-1.3542e-04,  8.0201e-01, -2.2346e-04]):  84%|▊| 1682/2000 [41:36<0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 21\u001b[0m\n\u001b[1;32m     17\u001b[0m uT, yT \u001b[39m=\u001b[39m u_ini, y_ini\n\u001b[1;32m     19\u001b[0m \u001b[39mfor\u001b[39;00m j \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(T):\n\u001b[1;32m     20\u001b[0m     \u001b[39m# Get optimal action from controller, take MSE of trajectory and reference signal\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m     u_pred, y_pred , cost \u001b[39m=\u001b[39m learner(ref\u001b[39m=\u001b[39;49mref, u_ini\u001b[39m=\u001b[39;49mu_ini, y_ini\u001b[39m=\u001b[39;49my_ini)\n\u001b[1;32m     22\u001b[0m     action \u001b[39m=\u001b[39m u_pred[:m]\n\u001b[1;32m     23\u001b[0m     obs \u001b[39m=\u001b[39m A\u001b[39m@y_ini\u001b[39m[\u001b[39m-\u001b[39mp:] \u001b[39m+\u001b[39m action\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/differentiable-deepc/src/controllers.py:325\u001b[0m, in \u001b[0;36mDDeePC.forward\u001b[0;34m(self, ref, u_ini, y_ini)\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstochastic:\n\u001b[1;32m    323\u001b[0m     params\u001b[39m.\u001b[39mappend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlam_y)\n\u001b[0;32m--> 325\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mQP_layer(\u001b[39m*\u001b[39;49mparams)\n\u001b[1;32m    326\u001b[0m \u001b[39minput\u001b[39m, output \u001b[39m=\u001b[39m out[\u001b[39m2\u001b[39m], out[\u001b[39m3\u001b[39m]\n\u001b[1;32m    328\u001b[0m traj_cost \u001b[39m=\u001b[39m \u001b[39minput\u001b[39m\u001b[39m.\u001b[39mT \u001b[39m@\u001b[39m R \u001b[39m@\u001b[39m \u001b[39minput\u001b[39m \u001b[39m+\u001b[39m (output \u001b[39m-\u001b[39m ref)\u001b[39m.\u001b[39mT \u001b[39m@\u001b[39m Q \u001b[39m@\u001b[39m (output \u001b[39m-\u001b[39m ref)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages/cvxpylayers-0.1.5-py3.10.egg/cvxpylayers/torch/cvxpylayer.py:153\u001b[0m, in \u001b[0;36mCvxpyLayer.forward\u001b[0;34m(self, solver_args, *params)\u001b[0m\n\u001b[1;32m    140\u001b[0m info \u001b[39m=\u001b[39m {}\n\u001b[1;32m    141\u001b[0m f \u001b[39m=\u001b[39m _CvxpyLayerFn(\n\u001b[1;32m    142\u001b[0m     param_order\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparam_order,\n\u001b[1;32m    143\u001b[0m     param_ids\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparam_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    151\u001b[0m     info\u001b[39m=\u001b[39minfo,\n\u001b[1;32m    152\u001b[0m )\n\u001b[0;32m--> 153\u001b[0m sol \u001b[39m=\u001b[39m f(\u001b[39m*\u001b[39;49mparams)\n\u001b[1;32m    154\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minfo \u001b[39m=\u001b[39m info\n\u001b[1;32m    155\u001b[0m \u001b[39mreturn\u001b[39;00m sol\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages/torch/autograd/function.py:506\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    503\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m torch\u001b[39m.\u001b[39m_C\u001b[39m.\u001b[39m_are_functorch_transforms_active():\n\u001b[1;32m    504\u001b[0m     \u001b[39m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[1;32m    505\u001b[0m     args \u001b[39m=\u001b[39m _functorch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[0;32m--> 506\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mapply(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    508\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39msetup_context \u001b[39m==\u001b[39m _SingleLevelFunction\u001b[39m.\u001b[39msetup_context:\n\u001b[1;32m    509\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m    510\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    511\u001b[0m         \u001b[39m'\u001b[39m\u001b[39m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    512\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mstaticmethod. For more details, please see \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    513\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mhttps://pytorch.org/docs/master/notes/extending.func.html\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages/cvxpylayers-0.1.5-py3.10.egg/cvxpylayers/torch/cvxpylayer.py:289\u001b[0m, in \u001b[0;36m_CvxpyLayerFn.<locals>._CvxpyLayerFnFn.forward\u001b[0;34m(ctx, *params)\u001b[0m\n\u001b[1;32m    287\u001b[0m start \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m    288\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 289\u001b[0m     xs, _, _, _, ctx\u001b[39m.\u001b[39mDT_batch \u001b[39m=\u001b[39m diffcp\u001b[39m.\u001b[39;49msolve_and_derivative_batch(\n\u001b[1;32m    290\u001b[0m         As, bs, cs, cone_dicts, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49msolver_args)\n\u001b[1;32m    291\u001b[0m \u001b[39mexcept\u001b[39;00m diffcp\u001b[39m.\u001b[39mSolverError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    292\u001b[0m     \u001b[39mprint\u001b[39m(\n\u001b[1;32m    293\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mPlease consider re-formulating your problem so that \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    294\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mit is always solvable or increasing the number of \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    295\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39msolver iterations.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages/diffcp/cone_program.py:80\u001b[0m, in \u001b[0;36msolve_and_derivative_batch\u001b[0;34m(As, bs, cs, cone_dicts, n_jobs_forward, n_jobs_backward, mode, warm_starts, **kwargs)\u001b[0m\n\u001b[1;32m     78\u001b[0m xs, ys, ss, Ds, DTs \u001b[39m=\u001b[39m [], [], [], [], []\n\u001b[1;32m     79\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(batch_size):\n\u001b[0;32m---> 80\u001b[0m     x, y, s, D, DT \u001b[39m=\u001b[39m solve_and_derivative(As[i], bs[i], cs[i],\n\u001b[1;32m     81\u001b[0m                                           cone_dicts[i], warm_starts[i], mode\u001b[39m=\u001b[39;49mmode, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     82\u001b[0m     xs \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m [x]\n\u001b[1;32m     83\u001b[0m     ys \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m [y]\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages/diffcp/cone_program.py:216\u001b[0m, in \u001b[0;36msolve_and_derivative\u001b[0;34m(A, b, c, cone_dict, warm_start, mode, solve_method, **kwargs)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msolve_and_derivative\u001b[39m(A, b, c, cone_dict, warm_start\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, mode\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mlsqr\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m    153\u001b[0m                          solve_method\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mSCS\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    154\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Solves a cone program, returns its derivative as an abstract linear map.\u001b[39;00m\n\u001b[1;32m    155\u001b[0m \n\u001b[1;32m    156\u001b[0m \u001b[39m    This function solves a convex cone program, with primal-dual problems\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[39m        SolverError: if the cone program is infeasible or unbounded.\u001b[39;00m\n\u001b[1;32m    215\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 216\u001b[0m     result \u001b[39m=\u001b[39m solve_and_derivative_internal(\n\u001b[1;32m    217\u001b[0m         A, b, c, cone_dict, warm_start\u001b[39m=\u001b[39;49mwarm_start, mode\u001b[39m=\u001b[39;49mmode,\n\u001b[1;32m    218\u001b[0m         solve_method\u001b[39m=\u001b[39;49msolve_method, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    219\u001b[0m     x \u001b[39m=\u001b[39m result[\u001b[39m\"\u001b[39m\u001b[39mx\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m    220\u001b[0m     y \u001b[39m=\u001b[39m result[\u001b[39m\"\u001b[39m\u001b[39my\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages/diffcp/cone_program.py:413\u001b[0m, in \u001b[0;36msolve_and_derivative_internal\u001b[0;34m(A, b, c, cone_dict, solve_method, warm_start, mode, raise_on_error, **kwargs)\u001b[0m\n\u001b[1;32m    411\u001b[0m     MT \u001b[39m=\u001b[39m M\u001b[39m.\u001b[39mT\n\u001b[1;32m    412\u001b[0m \u001b[39melif\u001b[39;00m mode \u001b[39min\u001b[39;00m (\u001b[39m\"\u001b[39m\u001b[39mlsqr\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mlsmr\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m--> 413\u001b[0m     M \u001b[39m=\u001b[39m _diffcp\u001b[39m.\u001b[39;49mM_operator(Q, cones_parsed, u, v, w)\n\u001b[1;32m    414\u001b[0m     MT \u001b[39m=\u001b[39m M\u001b[39m.\u001b[39mtranspose()\n\u001b[1;32m    416\u001b[0m pi_z \u001b[39m=\u001b[39m pi(z, cones)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "opt = optim.RMSprop(learner.parameters(), lr=1)\n",
    "pbar = tqdm(range(2000), ncols=150)\n",
    "criterion = torch.nn.MSELoss()\n",
    "scheduler = lr_scheduler.LinearLR(opt, start_factor=1.0, end_factor=0.01, total_iters=250)\n",
    "T = 20\n",
    "\n",
    "for i, count in enumerate(pbar):\n",
    "    \n",
    "    # Get random step response\n",
    "    ref = torch.Tensor(np.random.uniform(size=(3,), low=-10.0, high=10.0))\n",
    "    target_ref = torch.kron(torch.ones(T), ref)\n",
    "    ref = torch.kron(torch.ones(Tf), ref)\n",
    "    \n",
    "    # Get random initial condition from data\n",
    "    index = int(np.random.uniform(low=0, high=5))\n",
    "    u_ini, y_ini = torch.Tensor(ud[index:Tini + index, :].reshape((Tini*m,))), torch.Tensor(yd[index:Tini + index].reshape((Tini*p,)))\n",
    "    uT, yT = u_ini, y_ini\n",
    "\n",
    "    for j in range(T):\n",
    "        # Get optimal action from controller, take MSE of trajectory and reference signal\n",
    "        u_pred, y_pred , cost = learner(ref=ref, u_ini=u_ini, y_ini=y_ini)\n",
    "        action = u_pred[:m]\n",
    "        obs = A@y_ini[-p:] + action\n",
    "        uT, yT = torch.cat((uT, action), 0), torch.cat((yT, obs), 0)\n",
    "        y_ini, u_ini = yT[-p*Tini:], uT[-m*Tini:]\n",
    "\n",
    "    loss = criterion(input=yT[p*Tini:], target=target_ref)\n",
    "    opt.zero_grad()\n",
    "    loss.backward(retain_graph=True)\n",
    "    opt.step()\n",
    "    scheduler.step()\n",
    "    \n",
    "    pbar.set_description(f'Loss = {loss.item():.4f}, lam_y = {learner.lam_y.data.item():.3f}, q = {learner.q.data}, r = {learner.r.data}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c6e4e9f98eb68ad3b7c296f83d20e6de614cb42e90992a65aa266555a3137d0d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
